# Klever Model Registry

English | [中文](docs_zh/README.md)

Klever Model Registry is a cloud-native model registry that uses [Harbor](https://github.com/goharbor/harbor) as storage for training models. Users can use Klever Model Registry for model management, model extraction, model conversion and model serving.

Klever Model Registry is an open source project based on [Apache 2.0 open source licenses](https://www.apache.org/licenses/LICENSE-2.0) and is a sub-project of Klever Cloud-Native Machine Learning Platform. Klever Model Registry is referred to as Klever in the following paragraphs.

<!-- // TODO:
## Getting started

To get started, complete the [Klever Tutorial]() and go through our
[examples]().
-->

## Understanding Klever Model Registry

See the following topics and our [API Docs](https://kleveross.github.io/klever-model-registry/api/) to learn how to use Klever in your project:

- [Model Management](#Model-Management)
- [Model Extraction](#Model-Extraction)
- [Model Conversion](#Model-Conversion)
- [Model Serving](#Model-Serving)

## Model Management

Users can upload and download their models by calling the API, and Klever will push the model into Harbor through 'ormb'.

> `ormb` is another open-source project of Klever Cloud-Native Machine Learning Platform, which is an OCI(Open Container Initiative)-based registry that aims to distribute models and models' metadata by using the existed image registry. See the project [ORMB](https://github.com/kleveross/ormb) for more information.

Users can upload the model to Harbor by specifying the project name, model name and the version of the model. To satisfy the `ormb` specification, the model package must have `ormbfile.yaml`, in which stores some information about the model, such as frame, format, etc. (We will support generating the `ormbfile.yaml` automatically in the near future, coming soon!) . Klever has acted as agent for all Harbor requests, project can be created through Klever if there is no Harbor project.

## Model Extraction
When the model is pushed to Harbor, Klever will automatically create `ModelJob` to extract the model. `ModelJob` is a [CRD](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) defined by `Klever`. Klever will fill the blank of `ModelJob.Spec.Extraction` based on the format of the model. `ModelJob` will then generate a `Job` to execute model conversion. The current model extraction formats supported by Klever are:
- SavedModel
- ONNX
- GraphDef
- NetDef
- Keras H5
- CaffeModel
- TorchScript
- MXNetParams
- PMML

The image of the `Job` who generated by `ModelJob` will extract the model and push the updated `ormbfile.yaml` to Harbor. See the detail code here: [extract](/scripts/extract/extract.py).

## Model Conversion

The current model conversion formats supported by Klever are:
- MXNetParams converted to ONNX
- Keras H5 converted to SavedModel
- CaffeModel converted to NetDef
- NetDef converted to ONNX

Users can create `ModelJob` for model conversion by calling the API. The original format and target format of the model will be specified by `ModelJob.Spec.Conversion Mmdnn.From` and `ModelJob.Spec.Conversion.Mmdnn.To`. The image of the `Job` who generated by `ModelJob` will convert the model and push the updated `ormbfile.yaml` to Harbor. See the detail code here: [convert](/scripts/convert/base_convert/base_convert.py).

## Model Serving

Klever's model serving is based on [Seldon-Core](https://github.com/SeldonIO/seldon-core). Klever will create a `Seldon Deployment` when users deploy a model serving. The model will be downloaded in its `Init Container` via [ormb-storage-initializer](https://github.com/kleveross/ormb/blob/master/build/ormb-storage-initializer/Dockerfile). If the model's format is PMML, the [OpenScoring Image](/build/serving/openscoring/Dockerfile) will be used to start the serving pod; If the model format is supported by [Triton Server](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/model_repository.html#framework-model-definition), the [Triton Server Image](/build/serving/tensorrt/Dockerfile) will be used to start the serving pod, in which the image will automatically generate the [config.pbtxt](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_configuration.html#) file required by Triton Server through the information in `ormbfile.yaml`.

## Contributing to Klever

If you'd like to contribute to the Klever project, see the [Klever Contributor's Guide](/CONTRIBUTING.md).
